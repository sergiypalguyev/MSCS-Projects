VI Convergence True  gamma(1.0), theta(1e-08) reached at 4/1000 iterations in 10.4(s). delta=0.0, mean=0.3529, reward=0.416. rewards/iterations=0.504
PI Convergence True  gamma(1.0), theta(1e-08) reached at 6/1000 iterations in 0.02(s). delta=0.0, mean=0.3529, reward=0.416. rewards/iterations=0.401
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.035, avg.delta=0.039, avg.reward=0.343
VI Convergence True  gamma(1.0), theta(1e-08) reached at 4/1000 iterations in 10.62(s). delta=0.0, mean=0.3529, reward=0.416. rewards/iterations=0.504
PI Convergence True  gamma(1.0), theta(1e-08) reached at 6/1000 iterations in 0.02(s). delta=0.0, mean=0.3529, reward=0.416. rewards/iterations=0.401
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.024, avg.delta=0.041, avg.reward=0.354
VI Convergence True  gamma(1.0), theta(1e-08) reached at 4/1000 iterations in 9.01(s). delta=0.0, mean=0.3529, reward=0.416. rewards/iterations=0.504
PI Convergence True  gamma(1.0), theta(1e-08) reached at 6/1000 iterations in 0.02(s). delta=0.0, mean=0.3529, reward=0.416. rewards/iterations=0.401
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.025, avg.delta=0.035, avg.reward=0.324
VI Convergence True  gamma(1.0), theta(1e-08) reached at 4/1000 iterations in 8.93(s). delta=0.0, mean=0.3529, reward=0.416. rewards/iterations=0.504
PI Convergence True  gamma(1.0), theta(1e-08) reached at 6/1000 iterations in 0.01(s). delta=0.0, mean=0.3529, reward=0.416. rewards/iterations=0.401
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.027, avg.delta=0.04, avg.reward=0.355
VI Convergence True  gamma(1.0), theta(1e-08) reached at 4/1000 iterations in 9.17(s). delta=0.0, mean=0.3529, reward=0.416. rewards/iterations=0.504
PI Convergence True  gamma(1.0), theta(1e-08) reached at 6/1000 iterations in 0.01(s). delta=0.0, mean=0.3529, reward=0.416. rewards/iterations=0.401
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.036, avg.delta=0.04, avg.reward=0.341
VI Convergence True  gamma(1.0), theta(1e-08) reached at 4/1000 iterations in 8.77(s). delta=0.0, mean=0.3529, reward=0.416. rewards/iterations=0.504
PI Convergence True  gamma(1.0), theta(1e-08) reached at 6/1000 iterations in 0.02(s). delta=0.0, mean=0.3529, reward=0.416. rewards/iterations=0.401
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.024, avg.delta=0.037, avg.reward=0.322
VI Convergence True  gamma(1.0), theta(1e-08) reached at 5/1000 iterations in 12.96(s). delta=0.0, mean=0.3758, reward=0.398. rewards/iterations=0.453
PI Convergence True  gamma(1.0), theta(1e-08) reached at 1/1000 iterations in 0.01(s). delta=0.0, mean=0.0, reward=0.337. rewards/iterations=0.337
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.008, avg.delta=0.04, avg.reward=0.372
VI Convergence True  gamma(1.0), theta(1e-08) reached at 5/1000 iterations in 12.64(s). delta=0.0, mean=0.3758, reward=0.398. rewards/iterations=0.454
PI Convergence True  gamma(1.0), theta(1e-08) reached at 1/1000 iterations in 0.01(s). delta=0.0, mean=0.0, reward=0.337. rewards/iterations=0.337
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.009, avg.delta=0.042, avg.reward=0.341
VI Convergence True  gamma(1.0), theta(1e-08) reached at 5/1000 iterations in 11.92(s). delta=0.0, mean=0.3758, reward=0.398. rewards/iterations=0.454
PI Convergence True  gamma(1.0), theta(1e-08) reached at 1/1000 iterations in 0.01(s). delta=0.0, mean=0.0, reward=0.337. rewards/iterations=0.337
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.007, avg.delta=0.042, avg.reward=0.369
VI Convergence True  gamma(1.0), theta(1e-08) reached at 5/1000 iterations in 13.52(s). delta=0.0, mean=0.3758, reward=0.398. rewards/iterations=0.453
PI Convergence True  gamma(1.0), theta(1e-08) reached at 1/1000 iterations in 0.01(s). delta=0.0, mean=0.0, reward=0.337. rewards/iterations=0.337
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.008, avg.delta=0.044, avg.reward=0.379
VI Convergence True  gamma(1.0), theta(1e-08) reached at 5/1000 iterations in 11.84(s). delta=0.0, mean=0.3758, reward=0.398. rewards/iterations=0.454
PI Convergence True  gamma(1.0), theta(1e-08) reached at 1/1000 iterations in 0.01(s). delta=0.0, mean=0.0, reward=0.337. rewards/iterations=0.337
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.008, avg.delta=0.04, avg.reward=0.334
VI Convergence True  gamma(1.0), theta(1e-08) reached at 6/1000 iterations in 17.68(s). delta=0.0, mean=0.3877, reward=0.389. rewards/iterations=0.437
PI Convergence True  gamma(1.0), theta(1e-08) reached at 8/1000 iterations in 0.17(s). delta=0.0, mean=0.3877, reward=0.416. rewards/iterations=0.38
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.003, avg.delta=0.042, avg.reward=0.363
VI Convergence True  gamma(1.0), theta(1e-08) reached at 6/1000 iterations in 17.48(s). delta=0.0, mean=0.3877, reward=0.389. rewards/iterations=0.437
PI Convergence True  gamma(1.0), theta(1e-08) reached at 8/1000 iterations in 0.16(s). delta=0.0, mean=0.3877, reward=0.416. rewards/iterations=0.38
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.003, avg.delta=0.042, avg.reward=0.332
VI Convergence True  gamma(1.0), theta(1e-08) reached at 6/1000 iterations in 17.54(s). delta=0.0, mean=0.3877, reward=0.389. rewards/iterations=0.437
PI Convergence True  gamma(1.0), theta(1e-08) reached at 8/1000 iterations in 0.17(s). delta=0.0, mean=0.3877, reward=0.416. rewards/iterations=0.38
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.002, avg.delta=0.043, avg.reward=0.354
VI Convergence True  gamma(1.0), theta(1e-08) reached at 6/1000 iterations in 17.97(s). delta=0.0, mean=0.3877, reward=0.389. rewards/iterations=0.437
PI Convergence True  gamma(1.0), theta(1e-08) reached at 8/1000 iterations in 0.17(s). delta=0.0, mean=0.3877, reward=0.416. rewards/iterations=0.38
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.003, avg.delta=0.045, avg.reward=0.359
VI Convergence True  gamma(1.0), theta(1e-08) reached at 6/1000 iterations in 17.6(s). delta=0.0, mean=0.3877, reward=0.389. rewards/iterations=0.436
PI Convergence True  gamma(1.0), theta(1e-08) reached at 8/1000 iterations in 0.16(s). delta=0.0, mean=0.3877, reward=0.416. rewards/iterations=0.38
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.002, avg.delta=0.041, avg.reward=0.364
VI Convergence True  gamma(1.0), theta(1e-08) reached at 7/1000 iterations in 27.79(s). delta=0.0, mean=0.3938, reward=0.397. rewards/iterations=0.436
PI Convergence True  gamma(1.0), theta(1e-08) reached at 9/1000 iterations in 0.69(s). delta=0.0, mean=0.3938, reward=0.413. rewards/iterations=0.387
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.002, avg.delta=0.04, avg.reward=0.344
VI Convergence True  gamma(1.0), theta(1e-08) reached at 7/1000 iterations in 27.97(s). delta=0.0, mean=0.3938, reward=0.397. rewards/iterations=0.436
PI Convergence True  gamma(1.0), theta(1e-08) reached at 9/1000 iterations in 0.68(s). delta=0.0, mean=0.3938, reward=0.413. rewards/iterations=0.387
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.002, avg.delta=0.041, avg.reward=0.364
VI Convergence True  gamma(1.0), theta(1e-08) reached at 7/1000 iterations in 28.03(s). delta=0.0, mean=0.3938, reward=0.397. rewards/iterations=0.437
PI Convergence True  gamma(1.0), theta(1e-08) reached at 9/1000 iterations in 0.66(s). delta=0.0, mean=0.3938, reward=0.413. rewards/iterations=0.387
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.002, avg.delta=0.042, avg.reward=0.344
VI Convergence True  gamma(1.0), theta(1e-08) reached at 7/1000 iterations in 27.77(s). delta=0.0, mean=0.3938, reward=0.397. rewards/iterations=0.436
PI Convergence True  gamma(1.0), theta(1e-08) reached at 9/1000 iterations in 0.67(s). delta=0.0, mean=0.3938, reward=0.413. rewards/iterations=0.387
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.002, avg.delta=0.041, avg.reward=0.338
VI Convergence True  gamma(1.0), theta(1e-08) reached at 7/1000 iterations in 28.14(s). delta=0.0, mean=0.3938, reward=0.397. rewards/iterations=0.436
PI Convergence True  gamma(1.0), theta(1e-08) reached at 9/1000 iterations in 0.65(s). delta=0.0, mean=0.3938, reward=0.413. rewards/iterations=0.387
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.001, avg.delta=0.044, avg.reward=0.355
VI Convergence True  gamma(1.0), theta(1e-08) reached at 8/1000 iterations in 46.63(s). delta=0.0, mean=0.3969, reward=0.427. rewards/iterations=0.45
PI Convergence True  gamma(1.0), theta(1e-08) reached at 10/1000 iterations in 2.78(s). delta=0.0, mean=0.3969, reward=0.444. rewards/iterations=0.4
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.001, avg.delta=0.043, avg.reward=0.359
VI Convergence True  gamma(1.0), theta(1e-08) reached at 8/1000 iterations in 46.91(s). delta=0.0, mean=0.3969, reward=0.427. rewards/iterations=0.45
PI Convergence True  gamma(1.0), theta(1e-08) reached at 10/1000 iterations in 2.8(s). delta=0.0, mean=0.3969, reward=0.444. rewards/iterations=0.4
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.001, avg.delta=0.037, avg.reward=0.329
VI Convergence True  gamma(1.0), theta(1e-08) reached at 8/1000 iterations in 46.66(s). delta=0.0, mean=0.3969, reward=0.427. rewards/iterations=0.45
PI Convergence True  gamma(1.0), theta(1e-08) reached at 10/1000 iterations in 2.8(s). delta=0.0, mean=0.3969, reward=0.444. rewards/iterations=0.4
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.001, avg.delta=0.044, avg.reward=0.365
VI Convergence True  gamma(1.0), theta(1e-08) reached at 8/1000 iterations in 47.04(s). delta=0.0, mean=0.3969, reward=0.427. rewards/iterations=0.45
PI Convergence True  gamma(1.0), theta(1e-08) reached at 10/1000 iterations in 2.82(s). delta=0.0, mean=0.3969, reward=0.444. rewards/iterations=0.4
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.001, avg.delta=0.038, avg.reward=0.343
VI Convergence True  gamma(1.0), theta(1e-08) reached at 8/1000 iterations in 47.29(s). delta=0.0, mean=0.3969, reward=0.427. rewards/iterations=0.45
PI Convergence True  gamma(1.0), theta(1e-08) reached at 10/1000 iterations in 2.83(s). delta=0.0, mean=0.3969, reward=0.444. rewards/iterations=0.4
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.001, avg.delta=0.039, avg.reward=0.337
VI Convergence True  gamma(1.0), theta(1e-08) reached at 9/1000 iterations in 71.46(s). delta=0.0, mean=0.3984, reward=0.413. rewards/iterations=0.434
PI Convergence True  gamma(1.0), theta(1e-08) reached at 11/1000 iterations in 11.96(s). delta=0.0, mean=0.3984, reward=0.414. rewards/iterations=0.396
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.001, avg.delta=0.034, avg.reward=0.308
VI Convergence True  gamma(1.0), theta(1e-08) reached at 9/1000 iterations in 72.75(s). delta=0.0, mean=0.3984, reward=0.413. rewards/iterations=0.434
PI Convergence True  gamma(1.0), theta(1e-08) reached at 11/1000 iterations in 12.13(s). delta=0.0, mean=0.3984, reward=0.414. rewards/iterations=0.396
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.001, avg.delta=0.034, avg.reward=0.31
VI Convergence True  gamma(1.0), theta(1e-08) reached at 9/1000 iterations in 71.33(s). delta=0.0, mean=0.3984, reward=0.413. rewards/iterations=0.434
PI Convergence True  gamma(1.0), theta(1e-08) reached at 11/1000 iterations in 11.81(s). delta=0.0, mean=0.3984, reward=0.414. rewards/iterations=0.396
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.001, avg.delta=0.035, avg.reward=0.323
VI Convergence True  gamma(1.0), theta(1e-08) reached at 9/1000 iterations in 72.01(s). delta=0.0, mean=0.3984, reward=0.413. rewards/iterations=0.434
PI Convergence True  gamma(1.0), theta(1e-08) reached at 11/1000 iterations in 11.8(s). delta=0.0, mean=0.3984, reward=0.414. rewards/iterations=0.396
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.001, avg.delta=0.038, avg.reward=0.306
VI Convergence True  gamma(1.0), theta(1e-08) reached at 9/1000 iterations in 71.29(s). delta=0.0, mean=0.3984, reward=0.413. rewards/iterations=0.434
PI Convergence True  gamma(1.0), theta(1e-08) reached at 11/1000 iterations in 11.92(s). delta=0.0, mean=0.3984, reward=0.414. rewards/iterations=0.396
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.001, avg.delta=0.035, avg.reward=0.309
VI Convergence True  gamma(1.0), theta(1e-08) reached at 10/1000 iterations in 123.97(s). delta=0.0, mean=0.3992, reward=0.408. rewards/iterations=0.416
PI Convergence True  gamma(1.0), theta(1e-08) reached at 12/1000 iterations in 51.55(s). delta=0.0, mean=0.3992, reward=0.42. rewards/iterations=0.383
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.001, avg.delta=0.034, avg.reward=0.325
VI Convergence True  gamma(1.0), theta(1e-08) reached at 10/1000 iterations in 123.6(s). delta=0.0, mean=0.3992, reward=0.408. rewards/iterations=0.416
PI Convergence True  gamma(1.0), theta(1e-08) reached at 12/1000 iterations in 51.1(s). delta=0.0, mean=0.3992, reward=0.42. rewards/iterations=0.383
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.001, avg.delta=0.033, avg.reward=0.316
VI Convergence True  gamma(1.0), theta(1e-08) reached at 10/1000 iterations in 122.89(s). delta=0.0, mean=0.3992, reward=0.408. rewards/iterations=0.416
PI Convergence True  gamma(1.0), theta(1e-08) reached at 12/1000 iterations in 51.1(s). delta=0.0, mean=0.3992, reward=0.42. rewards/iterations=0.383
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.001, avg.delta=0.033, avg.reward=0.318
VI Convergence True  gamma(1.0), theta(1e-08) reached at 10/1000 iterations in 124.62(s). delta=0.0, mean=0.3992, reward=0.408. rewards/iterations=0.416
PI Convergence True  gamma(1.0), theta(1e-08) reached at 12/1000 iterations in 51.67(s). delta=0.0, mean=0.3992, reward=0.42. rewards/iterations=0.383
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.001, avg.delta=0.036, avg.reward=0.31
VI Convergence True  gamma(1.0), theta(1e-08) reached at 10/1000 iterations in 123.65(s). delta=0.0, mean=0.3992, reward=0.408. rewards/iterations=0.416
PI Convergence True  gamma(1.0), theta(1e-08) reached at 12/1000 iterations in 51.55(s). delta=0.0, mean=0.3992, reward=0.42. rewards/iterations=0.383
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.001, avg.delta=0.032, avg.reward=0.314
VI Convergence True  gamma(1.0), theta(1e-08) reached at 4/1000 iterations in 9.61(s). delta=0.0, mean=0.3529, reward=0.416. rewards/iterations=0.504
PI Convergence True  gamma(1.0), theta(1e-08) reached at 6/1000 iterations in 0.01(s). delta=0.0, mean=0.3529, reward=0.416. rewards/iterations=0.401
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.028, avg.delta=0.037, avg.reward=0.349
VI Convergence True  gamma(1.0), theta(1e-08) reached at 4/1000 iterations in 8.84(s). delta=0.0, mean=0.3529, reward=0.416. rewards/iterations=0.504
PI Convergence True  gamma(1.0), theta(1e-08) reached at 6/1000 iterations in 0.01(s). delta=0.0, mean=0.3529, reward=0.416. rewards/iterations=0.401
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.023, avg.delta=0.039, avg.reward=0.334
VI Convergence True  gamma(1.0), theta(1e-08) reached at 4/1000 iterations in 10.07(s). delta=0.0, mean=0.3529, reward=0.416. rewards/iterations=0.504
PI Convergence True  gamma(1.0), theta(1e-08) reached at 6/1000 iterations in 0.01(s). delta=0.0, mean=0.3529, reward=0.416. rewards/iterations=0.401
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.024, avg.delta=0.038, avg.reward=0.345
VI Convergence True  gamma(1.0), theta(1e-08) reached at 4/1000 iterations in 8.69(s). delta=0.0, mean=0.3529, reward=0.416. rewards/iterations=0.504
PI Convergence True  gamma(1.0), theta(1e-08) reached at 6/1000 iterations in 0.01(s). delta=0.0, mean=0.3529, reward=0.416. rewards/iterations=0.401
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.027, avg.delta=0.041, avg.reward=0.358
VI Convergence True  gamma(1.0), theta(1e-08) reached at 4/1000 iterations in 8.42(s). delta=0.0, mean=0.3529, reward=0.416. rewards/iterations=0.504
PI Convergence True  gamma(1.0), theta(1e-08) reached at 6/1000 iterations in 0.01(s). delta=0.0, mean=0.3529, reward=0.416. rewards/iterations=0.401
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.033, avg.delta=0.036, avg.reward=0.325
VI Convergence True  gamma(1.0), theta(1e-08) reached at 5/1000 iterations in 11.89(s). delta=0.0, mean=0.3758, reward=0.398. rewards/iterations=0.454
PI Convergence True  gamma(1.0), theta(1e-08) reached at 1/1000 iterations in 0.01(s). delta=0.0, mean=0.0, reward=0.337. rewards/iterations=0.337
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.008, avg.delta=0.043, avg.reward=0.356
VI Convergence True  gamma(1.0), theta(1e-08) reached at 5/1000 iterations in 11.85(s). delta=0.0, mean=0.3758, reward=0.398. rewards/iterations=0.454
PI Convergence True  gamma(1.0), theta(1e-08) reached at 1/1000 iterations in 0.01(s). delta=0.0, mean=0.0, reward=0.337. rewards/iterations=0.337
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.008, avg.delta=0.04, avg.reward=0.341
VI Convergence True  gamma(1.0), theta(1e-08) reached at 5/1000 iterations in 11.88(s). delta=0.0, mean=0.3758, reward=0.398. rewards/iterations=0.454
PI Convergence True  gamma(1.0), theta(1e-08) reached at 1/1000 iterations in 0.01(s). delta=0.0, mean=0.0, reward=0.337. rewards/iterations=0.337
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.008, avg.delta=0.039, avg.reward=0.352
VI Convergence True  gamma(1.0), theta(1e-08) reached at 5/1000 iterations in 12.0(s). delta=0.0, mean=0.3758, reward=0.398. rewards/iterations=0.454
PI Convergence True  gamma(1.0), theta(1e-08) reached at 1/1000 iterations in 0.01(s). delta=0.0, mean=0.0, reward=0.337. rewards/iterations=0.337
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.006, avg.delta=0.04, avg.reward=0.359
VI Convergence True  gamma(1.0), theta(1e-08) reached at 5/1000 iterations in 11.85(s). delta=0.0, mean=0.3758, reward=0.398. rewards/iterations=0.454
PI Convergence True  gamma(1.0), theta(1e-08) reached at 1/1000 iterations in 0.01(s). delta=0.0, mean=0.0, reward=0.337. rewards/iterations=0.337
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.011, avg.delta=0.039, avg.reward=0.335
VI Convergence True  gamma(1.0), theta(1e-08) reached at 6/1000 iterations in 17.72(s). delta=0.0, mean=0.3877, reward=0.389. rewards/iterations=0.437
PI Convergence True  gamma(1.0), theta(1e-08) reached at 8/1000 iterations in 0.17(s). delta=0.0, mean=0.3877, reward=0.416. rewards/iterations=0.38
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.003, avg.delta=0.045, avg.reward=0.367
VI Convergence True  gamma(1.0), theta(1e-08) reached at 6/1000 iterations in 17.87(s). delta=0.0, mean=0.3877, reward=0.389. rewards/iterations=0.437
PI Convergence True  gamma(1.0), theta(1e-08) reached at 8/1000 iterations in 0.18(s). delta=0.0, mean=0.3877, reward=0.416. rewards/iterations=0.38
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.004, avg.delta=0.041, avg.reward=0.363
VI Convergence True  gamma(1.0), theta(1e-08) reached at 6/1000 iterations in 17.12(s). delta=0.0, mean=0.3877, reward=0.389. rewards/iterations=0.437
PI Convergence True  gamma(1.0), theta(1e-08) reached at 8/1000 iterations in 0.16(s). delta=0.0, mean=0.3877, reward=0.416. rewards/iterations=0.38
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.002, avg.delta=0.044, avg.reward=0.379
VI Convergence True  gamma(1.0), theta(1e-08) reached at 6/1000 iterations in 17.15(s). delta=0.0, mean=0.3877, reward=0.389. rewards/iterations=0.437
PI Convergence True  gamma(1.0), theta(1e-08) reached at 8/1000 iterations in 0.17(s). delta=0.0, mean=0.3877, reward=0.416. rewards/iterations=0.38
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.003, avg.delta=0.043, avg.reward=0.362
VI Convergence True  gamma(1.0), theta(1e-08) reached at 6/1000 iterations in 17.1(s). delta=0.0, mean=0.3877, reward=0.389. rewards/iterations=0.437
PI Convergence True  gamma(1.0), theta(1e-08) reached at 8/1000 iterations in 0.17(s). delta=0.0, mean=0.3877, reward=0.416. rewards/iterations=0.38
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.004, avg.delta=0.043, avg.reward=0.341
VI Convergence True  gamma(1.0), theta(1e-08) reached at 7/1000 iterations in 26.94(s). delta=0.0, mean=0.3938, reward=0.397. rewards/iterations=0.436
PI Convergence True  gamma(1.0), theta(1e-08) reached at 9/1000 iterations in 0.69(s). delta=0.0, mean=0.3938, reward=0.413. rewards/iterations=0.387
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.002, avg.delta=0.039, avg.reward=0.312
VI Convergence True  gamma(1.0), theta(1e-08) reached at 7/1000 iterations in 27.21(s). delta=0.0, mean=0.3938, reward=0.397. rewards/iterations=0.436
PI Convergence True  gamma(1.0), theta(1e-08) reached at 9/1000 iterations in 0.71(s). delta=0.0, mean=0.3938, reward=0.413. rewards/iterations=0.387
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.002, avg.delta=0.04, avg.reward=0.346
VI Convergence True  gamma(1.0), theta(1e-08) reached at 7/1000 iterations in 27.39(s). delta=0.0, mean=0.3938, reward=0.397. rewards/iterations=0.436
PI Convergence True  gamma(1.0), theta(1e-08) reached at 9/1000 iterations in 0.69(s). delta=0.0, mean=0.3938, reward=0.413. rewards/iterations=0.387
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.002, avg.delta=0.038, avg.reward=0.31
VI Convergence True  gamma(1.0), theta(1e-08) reached at 7/1000 iterations in 27.73(s). delta=0.0, mean=0.3938, reward=0.397. rewards/iterations=0.436
PI Convergence True  gamma(1.0), theta(1e-08) reached at 9/1000 iterations in 0.67(s). delta=0.0, mean=0.3938, reward=0.413. rewards/iterations=0.387
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.002, avg.delta=0.037, avg.reward=0.304
VI Convergence True  gamma(1.0), theta(1e-08) reached at 7/1000 iterations in 27.12(s). delta=0.0, mean=0.3938, reward=0.397. rewards/iterations=0.436
PI Convergence True  gamma(1.0), theta(1e-08) reached at 9/1000 iterations in 0.68(s). delta=0.0, mean=0.3938, reward=0.413. rewards/iterations=0.387
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.001, avg.delta=0.042, avg.reward=0.346
VI Convergence True  gamma(1.0), theta(1e-08) reached at 8/1000 iterations in 45.79(s). delta=0.0, mean=0.3969, reward=0.427. rewards/iterations=0.45
PI Convergence True  gamma(1.0), theta(1e-08) reached at 10/1000 iterations in 2.88(s). delta=0.0, mean=0.3969, reward=0.444. rewards/iterations=0.4
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.001, avg.delta=0.041, avg.reward=0.347
VI Convergence True  gamma(1.0), theta(1e-08) reached at 8/1000 iterations in 45.59(s). delta=0.0, mean=0.3969, reward=0.427. rewards/iterations=0.45
PI Convergence True  gamma(1.0), theta(1e-08) reached at 10/1000 iterations in 2.88(s). delta=0.0, mean=0.3969, reward=0.444. rewards/iterations=0.4
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.002, avg.delta=0.039, avg.reward=0.351
VI Convergence True  gamma(1.0), theta(1e-08) reached at 8/1000 iterations in 45.4(s). delta=0.0, mean=0.3969, reward=0.427. rewards/iterations=0.45
PI Convergence True  gamma(1.0), theta(1e-08) reached at 10/1000 iterations in 2.9(s). delta=0.0, mean=0.3969, reward=0.444. rewards/iterations=0.4
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.001, avg.delta=0.04, avg.reward=0.363
VI Convergence True  gamma(1.0), theta(1e-08) reached at 8/1000 iterations in 47.42(s). delta=0.0, mean=0.3969, reward=0.427. rewards/iterations=0.45
PI Convergence True  gamma(1.0), theta(1e-08) reached at 10/1000 iterations in 2.92(s). delta=0.0, mean=0.3969, reward=0.444. rewards/iterations=0.4
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.001, avg.delta=0.038, avg.reward=0.336
VI Convergence True  gamma(1.0), theta(1e-08) reached at 8/1000 iterations in 45.51(s). delta=0.0, mean=0.3969, reward=0.427. rewards/iterations=0.45
PI Convergence True  gamma(1.0), theta(1e-08) reached at 10/1000 iterations in 2.84(s). delta=0.0, mean=0.3969, reward=0.444. rewards/iterations=0.4
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.001, avg.delta=0.039, avg.reward=0.31
VI Convergence True  gamma(1.0), theta(1e-08) reached at 9/1000 iterations in 71.21(s). delta=0.0, mean=0.3984, reward=0.413. rewards/iterations=0.434
PI Convergence True  gamma(1.0), theta(1e-08) reached at 11/1000 iterations in 12.16(s). delta=0.0, mean=0.3984, reward=0.414. rewards/iterations=0.396
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.001, avg.delta=0.038, avg.reward=0.338
VI Convergence True  gamma(1.0), theta(1e-08) reached at 9/1000 iterations in 71.38(s). delta=0.0, mean=0.3984, reward=0.413. rewards/iterations=0.434
PI Convergence True  gamma(1.0), theta(1e-08) reached at 11/1000 iterations in 13.53(s). delta=0.0, mean=0.3984, reward=0.414. rewards/iterations=0.396
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.001, avg.delta=0.039, avg.reward=0.355
VI Convergence True  gamma(1.0), theta(1e-08) reached at 9/1000 iterations in 71.52(s). delta=0.0, mean=0.3984, reward=0.413. rewards/iterations=0.434
PI Convergence True  gamma(1.0), theta(1e-08) reached at 11/1000 iterations in 12.1(s). delta=0.0, mean=0.3984, reward=0.414. rewards/iterations=0.396
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.001, avg.delta=0.041, avg.reward=0.344
VI Convergence True  gamma(1.0), theta(1e-08) reached at 9/1000 iterations in 76.79(s). delta=0.0, mean=0.3984, reward=0.413. rewards/iterations=0.434
PI Convergence True  gamma(1.0), theta(1e-08) reached at 11/1000 iterations in 13.07(s). delta=0.0, mean=0.3984, reward=0.414. rewards/iterations=0.396
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.001, avg.delta=0.033, avg.reward=0.326
VI Convergence True  gamma(1.0), theta(1e-08) reached at 9/1000 iterations in 77.89(s). delta=0.0, mean=0.3984, reward=0.413. rewards/iterations=0.434
PI Convergence True  gamma(1.0), theta(1e-08) reached at 11/1000 iterations in 12.46(s). delta=0.0, mean=0.3984, reward=0.414. rewards/iterations=0.396
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.001, avg.delta=0.04, avg.reward=0.347
VI Convergence True  gamma(1.0), theta(1e-08) reached at 10/1000 iterations in 131.15(s). delta=0.0, mean=0.3992, reward=0.408. rewards/iterations=0.416
PI Convergence True  gamma(1.0), theta(1e-08) reached at 12/1000 iterations in 52.56(s). delta=0.0, mean=0.3992, reward=0.42. rewards/iterations=0.383
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.001, avg.delta=0.033, avg.reward=0.295
VI Convergence True  gamma(1.0), theta(1e-08) reached at 10/1000 iterations in 135.09(s). delta=0.0, mean=0.3992, reward=0.408. rewards/iterations=0.416
PI Convergence True  gamma(1.0), theta(1e-08) reached at 12/1000 iterations in 55.27(s). delta=0.0, mean=0.3992, reward=0.42. rewards/iterations=0.383
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.001, avg.delta=0.034, avg.reward=0.321
VI Convergence True  gamma(1.0), theta(1e-08) reached at 10/1000 iterations in 130.56(s). delta=0.0, mean=0.3992, reward=0.408. rewards/iterations=0.416
PI Convergence True  gamma(1.0), theta(1e-08) reached at 12/1000 iterations in 53.42(s). delta=0.0, mean=0.3992, reward=0.42. rewards/iterations=0.383
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.001, avg.delta=0.037, avg.reward=0.33
VI Convergence True  gamma(1.0), theta(1e-08) reached at 10/1000 iterations in 128.95(s). delta=0.0, mean=0.3992, reward=0.408. rewards/iterations=0.416
PI Convergence True  gamma(1.0), theta(1e-08) reached at 12/1000 iterations in 53.28(s). delta=0.0, mean=0.3992, reward=0.42. rewards/iterations=0.383
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.001, avg.delta=0.039, avg.reward=0.342
VI Convergence True  gamma(1.0), theta(1e-08) reached at 10/1000 iterations in 131.35(s). delta=0.0, mean=0.3992, reward=0.408. rewards/iterations=0.416
PI Convergence True  gamma(1.0), theta(1e-08) reached at 12/1000 iterations in 55.45(s). delta=0.0, mean=0.3992, reward=0.42. rewards/iterations=0.383
Q epsilon eMax(0.5) eMin(0.3) eDecay(0.001) Convergence False in 1000 episodes. avg.error=0.001, avg.delta=0.032, avg.reward=0.314
